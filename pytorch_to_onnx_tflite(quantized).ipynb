{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_to_onnx_tflite(quantized).ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ac210722ef4b415ca5f479a18a831db1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a5237be7062f47bd8b8de131c86be368",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_2d1cd68153c44a2c893f0eedb4cf2e35",
              "IPY_MODEL_7f992445eaf74b9db2d2e6f9e8094a1e",
              "IPY_MODEL_715dacb690694d5181ef70e40d7f7b28"
            ]
          }
        },
        "a5237be7062f47bd8b8de131c86be368": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2d1cd68153c44a2c893f0eedb4cf2e35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fe9a881c9f674151a561c1d3b77275e5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4de901b89c1a4e91b82d934fc9c4a5bc"
          }
        },
        "7f992445eaf74b9db2d2e6f9e8094a1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_df93099fcbdc4cdb9c742738f20e2c71",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 14212972,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 14212972,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_69f2a243a288494fb85578de7d02f4b6"
          }
        },
        "715dacb690694d5181ef70e40d7f7b28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_badc28a6da0049b7ad5418ffd675a8a6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 62.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e4da87a88803425997cdb0c8de0da384"
          }
        },
        "fe9a881c9f674151a561c1d3b77275e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4de901b89c1a4e91b82d934fc9c4a5bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "df93099fcbdc4cdb9c742738f20e2c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "69f2a243a288494fb85578de7d02f4b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "badc28a6da0049b7ad5418ffd675a8a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e4da87a88803425997cdb0c8de0da384": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyadla-sys/pytorch_2_tflite/blob/main/pytorch_to_onnx_tflite(quantized).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install ONNX and ONNX runtime"
      ],
      "metadata": {
        "id": "OF42Y3YaWrbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "!pip install onnxruntime\n",
        "# Some standard imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.onnx\n",
        "import torchvision.models as models\n",
        "import onnx\n",
        "import onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma98ov9Fzftx",
        "outputId": "806a4f03-40eb-4ae8-9454-b160c7dcb8fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.11.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 13.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.21.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->onnx) (1.15.0)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.11.0\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 12.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (1.21.5)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime) (1.15.0)\n",
            "Installing collected packages: onnxruntime\n",
            "Successfully installed onnxruntime-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load mobilenetV2 from torch models"
      ],
      "metadata": {
        "id": "gL9nGbtlW9Wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.mobilenet_v2(pretrained=True)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "iVFwSNmHztHY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ac210722ef4b415ca5f479a18a831db1",
            "a5237be7062f47bd8b8de131c86be368",
            "2d1cd68153c44a2c893f0eedb4cf2e35",
            "7f992445eaf74b9db2d2e6f9e8094a1e",
            "715dacb690694d5181ef70e40d7f7b28",
            "fe9a881c9f674151a561c1d3b77275e5",
            "4de901b89c1a4e91b82d934fc9c4a5bc",
            "df93099fcbdc4cdb9c742738f20e2c71",
            "69f2a243a288494fb85578de7d02f4b6",
            "badc28a6da0049b7ad5418ffd675a8a6",
            "e4da87a88803425997cdb0c8de0da384"
          ]
        },
        "outputId": "89f3623b-9c0f-40ef-e8be-a9dd941c2f80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ac210722ef4b415ca5f479a18a831db1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MobileNetV2(\n",
              "  (features): Sequential(\n",
              "    (0): ConvNormActivation(\n",
              "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "    (1): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (2): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (3): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (9): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (10): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (11): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (12): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (13): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (14): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (15): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (16): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (17): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (18): ConvNormActivation(\n",
              "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.2, inplace=False)\n",
              "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert from pytorch to onnx"
      ],
      "metadata": {
        "id": "b4kR0xRGXOM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 1\n",
        "IMAGE_SIZE = 224\n",
        "# Input to the model\n",
        "x = torch.randn(BATCH_SIZE, 3, 224, 224, requires_grad=True)\n",
        "torch_out = model(x)\n",
        "\n",
        "# Export the model\n",
        "torch.onnx.export(model,                     # model being run\n",
        "                  x,                         # model input (or a tuple for multiple inputs)\n",
        "                  \"mobilenet_v2.onnx\",       # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,        # store the trained parameter weights inside the model file\n",
        "                  opset_version=10,          # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                  input_names = ['input'],   # the model's input names\n",
        "                  output_names = ['output'], # the model's output names\n",
        "                  dynamic_axes={'input' : {0 : 'BATCH_SIZE'},    # variable length axes\n",
        "                                'output' : {0 : 'BATCH_SIZE'}})"
      ],
      "metadata": {
        "id": "FU91JNA9zvNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model = onnx.load(\"mobilenet_v2.onnx\")\n",
        "onnx.checker.check_model(onnx_model)"
      ],
      "metadata": {
        "id": "p2XeU2w2z294"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Compare ONNX Runtime and Pytorch results"
      ],
      "metadata": {
        "id": "tKkajaAyXjeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ort_session = onnxruntime.InferenceSession(\"mobilenet_v2.onnx\")\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "# compute ONNX Runtime output prediction\n",
        "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
        "ort_outs = ort_session.run(None, ort_inputs)\n",
        "\n",
        "# compare ONNX Runtime and PyTorch results\n",
        "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
        "\n",
        "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
      ],
      "metadata": {
        "id": "a0FGMK0Fz5yo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9128ebcf-839c-4dc9-970f-ba3e02e36190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert from Onnx to TF saved model"
      ],
      "metadata": {
        "id": "FQB1Q8JbX2As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx-tf\n",
        "\n",
        "from onnx_tf.backend import prepare\n",
        "import onnx\n",
        "\n",
        "onnx_model_path = 'mobilenet_v2.onnx'\n",
        "tf_model_path = 'model_tf'\n",
        "\n",
        "onnx_model = onnx.load(onnx_model_path)\n",
        "tf_rep = prepare(onnx_model)\n",
        "tf_rep.export_graph(tf_model_path)"
      ],
      "metadata": {
        "id": "dBmONa3x1FZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a5eb2c-0bd0-46e3-9eed-0c1659e575cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx-tf\n",
            "  Downloading onnx_tf-1.9.0-py3-none-any.whl (222 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▌                              | 10 kB 31.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 20 kB 24.6 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 30 kB 18.3 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 40 kB 16.3 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 51 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 61 kB 11.6 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 71 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 81 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 92 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 102 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 112 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 122 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 133 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 143 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 153 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 163 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 174 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 184 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 194 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 204 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 215 kB 13.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 222 kB 13.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from onnx-tf) (3.13)\n",
            "Requirement already satisfied: onnx>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from onnx-tf) (1.11.0)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 70.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.9.0->onnx-tf) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.9.0->onnx-tf) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.9.0->onnx-tf) (1.21.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->onnx>=1.9.0->onnx-tf) (1.15.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->onnx-tf) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons, onnx-tf\n",
            "Successfully installed onnx-tf-1.9.0 tensorflow-addons-0.16.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model_tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: model_tf/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert from TF saved model to TFLite(float32) model"
      ],
      "metadata": {
        "id": "Waje2OHsX_Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "saved_model_dir = 'model_tf'\n",
        "tflite_model_path = 'mobilenet_v2_float32.tflite'\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open(tflite_model_path, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKq3zZfk1qwk",
        "outputId": "36a05a91-a7c6-4c4a-fa3a-8aebd6a61b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run inference on TFLite(float32) with random data"
      ],
      "metadata": {
        "id": "CsJDnIA1Yo8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tflite_model_path = '/content/mobilenet_v2_float32.tflite'\n",
        "# Load the TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model on random input data\n",
        "input_shape = input_details[0]['shape']\n",
        "print(input_shape)\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# get_tensor() returns a copy of the tensor data\n",
        "# use tensor() in order to get a pointer to the tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Predicted value for [0, 1] normalization. Label index: {}, confidence: {:2.0f}%\"\n",
        "      .format(np.argmax(output_data), \n",
        "              100 * output_data[0][np.argmax(output_data)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD9dfODY2XH8",
        "outputId": "4faf2430-8b2a-412f-944d-ef395f96c479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1   3 224 224]\n",
            "Predicted value for [0, 1] normalization. Label index: 892, confidence: 572%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run inference on TFLite(float32) with image data"
      ],
      "metadata": {
        "id": "jX0VA1kJbsVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /content/cats_and_dogs_filtered.zip\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seyq_VnYhk7g",
        "outputId": "454717db-9fb2-4eb3-a197-777cf75195a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-04 02:09:26--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.127.128, 142.251.18.128, 142.250.145.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.127.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/content/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/content/cats_and_d 100%[===================>]  65.43M   148MB/s    in 0.4s    \n",
            "\n",
            "2022-03-04 02:09:26 (148 MB/s) - ‘/content/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "NxDavT_Bh08y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tflite_model_path = '/content/mobilenet_v2_float32.tflite'\n",
        "\n",
        "#tflite_model_path = '/content/model_float32.tflite'\n",
        "# Load the TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "print(\"== Input details ==\")\n",
        "print(\"name:\", interpreter.get_input_details()[0]['name'])\n",
        "print(\"shape:\", interpreter.get_input_details()[0]['shape'])\n",
        "print(\"type:\", interpreter.get_input_details()[0]['dtype'])\n",
        "\n",
        "print(\"\\nDUMP INPUT\")\n",
        "print(interpreter.get_input_details()[0])\n",
        "\n",
        "print(\"\\n== Output details ==\")\n",
        "print(\"name:\", interpreter.get_output_details()[0]['name'])\n",
        "print(\"shape:\", interpreter.get_output_details()[0]['shape'])\n",
        "print(\"type:\", interpreter.get_output_details()[0]['dtype'])\n",
        "\n",
        "print(\"\\nDUMP OUTPUT\")\n",
        "print(interpreter.get_output_details()[0])\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model on image  data\n",
        "input_shape = input_details[0]['shape']\n",
        "#print(input_shape)\n",
        "image = tf.io.read_file('/content/cats_and_dogs_filtered/validation/cats/cat.2000.jpg')\n",
        "\n",
        "image = tf.io.decode_jpeg(image, channels=3)\n",
        "image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "image = tf.reshape(image,[3,IMAGE_SIZE,IMAGE_SIZE])\n",
        "image = tf.expand_dims(image, 0)\n",
        "print(\"Real image shape\")\n",
        "print(image.shape)\n",
        "#print(image)\n",
        "interpreter.set_tensor(input_details[0]['index'], image)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# get_tensor() returns a copy of the tensor data\n",
        "# use tensor() in order to get a pointer to the tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Predicted value . Label index: {}, confidence: {:2.0f}%\"\n",
        "      .format(np.argmax(output_data), \n",
        "              100 * output_data[0][np.argmax(output_data)]))"
      ],
      "metadata": {
        "id": "lhlCPBDO205F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55b150da-e695-4288-cafb-f2a8e287e6da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Input details ==\n",
            "name: serving_default_input:0\n",
            "shape: [  1   3 224 224]\n",
            "type: <class 'numpy.float32'>\n",
            "\n",
            "DUMP INPUT\n",
            "{'name': 'serving_default_input:0', 'index': 0, 'shape': array([  1,   3, 224, 224], dtype=int32), 'shape_signature': array([ -1,   3, 224, 224], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "\n",
            "== Output details ==\n",
            "name: PartitionedCall:0\n",
            "shape: [   1 1000]\n",
            "type: <class 'numpy.float32'>\n",
            "\n",
            "DUMP OUTPUT\n",
            "{'name': 'PartitionedCall:0', 'index': 346, 'shape': array([   1, 1000], dtype=int32), 'shape_signature': array([  -1, 1000], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "Real image shape\n",
            "(1, 3, 224, 224)\n",
            "Predicted value . Label index: 530, confidence: 1936%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Convert from TF saved model to TFLite(quantized) model"
      ],
      "metadata": {
        "id": "gys1dUne4SK0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9ydAmHGHUZl",
        "outputId": "82854d08-cef2-4ba9-d743-ad5d7b6d9958"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n",
            "(1, 3, 224, 224)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "# A generator that provides a representative dataset\n",
        "import tensorflow as tf\n",
        "saved_model_dir = 'model_tf'\n",
        "#flowers_dir = '/content/images'\n",
        "def representative_data_gen():\n",
        "  dataset_list = tf.data.Dataset.list_files('/content/cats_and_dogs_filtered/train' + '/*/*')\n",
        "  for i in range(100):\n",
        "    image = next(iter(dataset_list))\n",
        "    image = tf.io.read_file(image)\n",
        "    image = tf.io.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "    image = tf.reshape(image,[3,IMAGE_SIZE,IMAGE_SIZE])\n",
        "    image = tf.cast(image / 127., tf.float32)\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    print(image.shape)\n",
        "    yield [image]\n",
        "     \n",
        "#converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) \n",
        "# This enables quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# This sets the representative dataset for quantization\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# This ensures that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "# These set the input and output tensors to uint8 (added in r2.3)\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('mobilenet_v2_1.0_224_quant.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run inference on TFLite(quantized) with image data"
      ],
      "metadata": {
        "id": "tY2yaCnBe6UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tflite_model_path = '/content/mobilenet_v2_1.0_224_quant.tflite'\n",
        "#tflite_model_path = '/content/model_float32.tflite'\n",
        "# Load the TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "print(\"== Input details ==\")\n",
        "print(\"name:\", interpreter.get_input_details()[0]['name'])\n",
        "print(\"shape:\", interpreter.get_input_details()[0]['shape'])\n",
        "print(\"type:\", interpreter.get_input_details()[0]['dtype'])\n",
        " \n",
        "\n",
        "print(\"\\nDUMP INPUT\")\n",
        "print(interpreter.get_input_details()[0])\n",
        "\n",
        "print(\"\\n== Output details ==\")\n",
        "print(\"name:\", interpreter.get_output_details()[0]['name'])\n",
        "print(\"shape:\", interpreter.get_output_details()[0]['shape'])\n",
        "print(\"type:\", interpreter.get_output_details()[0]['dtype'])\n",
        "\n",
        "print(\"\\nDUMP OUTPUT\")\n",
        "print(interpreter.get_output_details()[0])\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "test_details = interpreter.get_input_details()[0]\n",
        "scale, zero_point = test_details['quantization']\n",
        "print(scale)\n",
        "print(zero_point)\n",
        "# Test the model on image  data\n",
        "input_shape = input_details[0]['shape']\n",
        "#print(input_shape)\n",
        "#image = tf.io.read_file('/content/169371301_d9b91a2a42.jpg')\n",
        "#image = tf.io.read_file('/content/istockphoto-472306883-1024x1024.jpg')\n",
        "#image = tf.io.read_file('/content/images/Car-PNG-HD.png')\n",
        "image = tf.io.read_file('/content/cats_and_dogs_filtered/validation/cats/cat.2000.jpg')\n",
        "\n",
        "image = tf.io.decode_jpeg(image, channels=3)\n",
        "#print(image.shape)     \n",
        "image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "image = tf.reshape(image,[3,IMAGE_SIZE,IMAGE_SIZE])\n",
        "#print(image.shape)\n",
        "image = tf.cast(image / 127., tf.int8)\n",
        "#image = tf.cast(image , tf.float32)\n",
        "image = tf.expand_dims(image, 0)\n",
        "print(\"Real image shape\")\n",
        "print(image.shape)\n",
        "#image = np.int8(image)\n",
        "#print(image)\n",
        "#image = np.float32(image / scale + zero_point)\n",
        "interpreter.set_tensor(input_details[0]['index'], image)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# get_tensor() returns a copy of the tensor data\n",
        "# use tensor() in order to get a pointer to the tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Predicted value . Label index: {}, confidence: {:2.0f}%\"\n",
        "      .format(np.argmax(output_data), \n",
        "              100 * output_data[0][np.argmax(output_data)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWAJCkXLOs4r",
        "outputId": "07fd7bc8-4189-465e-f5fb-024030322000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Input details ==\n",
            "name: serving_default_input:0\n",
            "shape: [  1   3 224 224]\n",
            "type: <class 'numpy.int8'>\n",
            "\n",
            "DUMP INPUT\n",
            "{'name': 'serving_default_input:0', 'index': 0, 'shape': array([  1,   3, 224, 224], dtype=int32), 'shape_signature': array([ -1,   3, 224, 224], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.007874015718698502, -128), 'quantization_parameters': {'scales': array([0.00787402], dtype=float32), 'zero_points': array([-128], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "\n",
            "== Output details ==\n",
            "name: PartitionedCall:0\n",
            "shape: [   1 1000]\n",
            "type: <class 'numpy.int8'>\n",
            "\n",
            "DUMP OUTPUT\n",
            "{'name': 'PartitionedCall:0', 'index': 380, 'shape': array([   1, 1000], dtype=int32), 'shape_signature': array([  -1, 1000], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.09701892733573914, -25), 'quantization_parameters': {'scales': array([0.09701893], dtype=float32), 'zero_points': array([-25], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "0.007874015718698502\n",
            "-128\n",
            "Real image shape\n",
            "(1, 3, 224, 224)\n",
            "Predicted value . Label index: 846, confidence: 1200%\n"
          ]
        }
      ]
    }
  ]
}