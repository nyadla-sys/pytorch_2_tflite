{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_to_onnx_to_tflite(quantized)_with_imagedata.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "802b8e5e42364ada8c76dfa5b4a9b1d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_01c43c4a0662440786e81c81cc3c36cf",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ce15bccfd411489caedd7dcf5b174beb",
              "IPY_MODEL_933f2f7c436b4ef58f62b005632cf3ab",
              "IPY_MODEL_74c2202b36374970b30d36c5a71b5eef"
            ]
          }
        },
        "01c43c4a0662440786e81c81cc3c36cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce15bccfd411489caedd7dcf5b174beb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_05b6f866696e4b1882a2f080a5d051c6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8a427b88cbc24774ab77eac6845844a9"
          }
        },
        "933f2f7c436b4ef58f62b005632cf3ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_caa1b8433a0e435e9e2e2f503106b6eb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 14212972,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 14212972,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4ebbe11310844332af10c082f2223c54"
          }
        },
        "74c2202b36374970b30d36c5a71b5eef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_cc311d64d885453f83b10de10d9d19e7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 33.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9efd80c8c2154262befc3c5e9de12508"
          }
        },
        "05b6f866696e4b1882a2f080a5d051c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8a427b88cbc24774ab77eac6845844a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "caa1b8433a0e435e9e2e2f503106b6eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4ebbe11310844332af10c082f2223c54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc311d64d885453f83b10de10d9d19e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9efd80c8c2154262befc3c5e9de12508": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyadla-sys/pytorch_2_tflite/blob/main/pytorch_to_onnx_to_tflite(quantized)_with_imagedata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install ONNX and ONNX runtime"
      ],
      "metadata": {
        "id": "OF42Y3YaWrbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "!pip install onnxruntime\n",
        "# Some standard imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.onnx\n",
        "import torchvision.models as models\n",
        "import onnx\n",
        "import onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma98ov9Fzftx",
        "outputId": "e3c9b559-1197-48c6-99c9-982b401ac0ee"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.11.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->onnx) (1.15.0)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.11.0\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (1.21.5)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime) (1.15.0)\n",
            "Installing collected packages: onnxruntime\n",
            "Successfully installed onnxruntime-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load mobilenetV2 from torch models"
      ],
      "metadata": {
        "id": "gL9nGbtlW9Wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.mobilenet_v2(pretrained=True)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "iVFwSNmHztHY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "802b8e5e42364ada8c76dfa5b4a9b1d7",
            "01c43c4a0662440786e81c81cc3c36cf",
            "ce15bccfd411489caedd7dcf5b174beb",
            "933f2f7c436b4ef58f62b005632cf3ab",
            "74c2202b36374970b30d36c5a71b5eef",
            "05b6f866696e4b1882a2f080a5d051c6",
            "8a427b88cbc24774ab77eac6845844a9",
            "caa1b8433a0e435e9e2e2f503106b6eb",
            "4ebbe11310844332af10c082f2223c54",
            "cc311d64d885453f83b10de10d9d19e7",
            "9efd80c8c2154262befc3c5e9de12508"
          ]
        },
        "outputId": "316e90c4-2a1e-4bcf-8eb2-c054a4f50c4d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "802b8e5e42364ada8c76dfa5b4a9b1d7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MobileNetV2(\n",
              "  (features): Sequential(\n",
              "    (0): ConvNormActivation(\n",
              "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "    (1): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (2): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (3): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (9): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (10): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (11): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (12): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (13): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (14): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (15): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (16): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (17): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (18): ConvNormActivation(\n",
              "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.2, inplace=False)\n",
              "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert from pytorch to onnx"
      ],
      "metadata": {
        "id": "b4kR0xRGXOM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 1\n",
        "IMAGE_SIZE = 224\n",
        "# Input to the model\n",
        "x = torch.randn(BATCH_SIZE, 3, 224, 224, requires_grad=True)\n",
        "torch_out = model(x)\n",
        "\n",
        "# Export the model\n",
        "torch.onnx.export(model,                     # model being run\n",
        "                  x,                         # model input (or a tuple for multiple inputs)\n",
        "                  \"mobilenet_v2.onnx\",       # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,        # store the trained parameter weights inside the model file\n",
        "                  opset_version=10,          # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                  input_names = ['input'],   # the model's input names\n",
        "                  output_names = ['output'], # the model's output names\n",
        "                  dynamic_axes={'input' : {0 : 'BATCH_SIZE'},    # variable length axes\n",
        "                                'output' : {0 : 'BATCH_SIZE'}})"
      ],
      "metadata": {
        "id": "FU91JNA9zvNB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model = onnx.load(\"mobilenet_v2.onnx\")\n",
        "onnx.checker.check_model(onnx_model)"
      ],
      "metadata": {
        "id": "p2XeU2w2z294"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Compare ONNX Runtime and Pytorch results"
      ],
      "metadata": {
        "id": "tKkajaAyXjeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ort_session = onnxruntime.InferenceSession(\"mobilenet_v2.onnx\")\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "# compute ONNX Runtime output prediction\n",
        "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
        "ort_outs = ort_session.run(None, ort_inputs)\n",
        "\n",
        "# compare ONNX Runtime and PyTorch results\n",
        "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
        "\n",
        "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
      ],
      "metadata": {
        "id": "a0FGMK0Fz5yo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55593157-ac83-4216-d158-16c14fea066c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert from Onnx to TF saved model"
      ],
      "metadata": {
        "id": "FQB1Q8JbX2As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx-tf\n",
        "\n",
        "from onnx_tf.backend import prepare\n",
        "import onnx\n",
        "\n",
        "onnx_model_path = 'mobilenet_v2.onnx'\n",
        "tf_model_path = 'model_tf'\n",
        "\n",
        "onnx_model = onnx.load(onnx_model_path)\n",
        "tf_rep = prepare(onnx_model)\n",
        "tf_rep.export_graph(tf_model_path)"
      ],
      "metadata": {
        "id": "dBmONa3x1FZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "255f1ca2-8858-4f61-8abf-613cf0b944f7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx-tf\n",
            "  Downloading onnx_tf-1.9.0-py3-none-any.whl (222 kB)\n",
            "\u001b[K     |████████████████████████████████| 222 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from onnx-tf) (3.13)\n",
            "Requirement already satisfied: onnx>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from onnx-tf) (1.11.0)\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 35.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.9.0->onnx-tf) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.9.0->onnx-tf) (1.21.5)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.9.0->onnx-tf) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->onnx>=1.9.0->onnx-tf) (1.15.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->onnx-tf) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons, onnx-tf\n",
            "Successfully installed onnx-tf-1.9.0 tensorflow-addons-0.16.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model_tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: model_tf/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert from TF saved model to TFLite(float32) model"
      ],
      "metadata": {
        "id": "Waje2OHsX_Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "saved_model_dir = 'model_tf'\n",
        "tflite_model_path = 'mobilenet_v2_float32.tflite'\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open(tflite_model_path, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKq3zZfk1qwk",
        "outputId": "f685a7c0-2bf8-49db-e220-543ab0c10388"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run inference on TFLite(float32) with random data"
      ],
      "metadata": {
        "id": "CsJDnIA1Yo8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tflite_model_path = '/content/mobilenet_v2_float32.tflite'\n",
        "# Load the TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model on random input data\n",
        "input_shape = input_details[0]['shape']\n",
        "print(input_shape)\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# get_tensor() returns a copy of the tensor data\n",
        "# use tensor() in order to get a pointer to the tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Predicted value for [0, 1] normalization. Label index: {}, confidence: {:2.0f}%\"\n",
        "      .format(np.argmax(output_data), \n",
        "              100 * output_data[0][np.argmax(output_data)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD9dfODY2XH8",
        "outputId": "8937c5cb-005e-4acb-d512-e3d956e9a8ed"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1   3 224 224]\n",
            "Predicted value for [0, 1] normalization. Label index: 892, confidence: 565%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run inference on TFLite(float32) with image data"
      ],
      "metadata": {
        "id": "jX0VA1kJbsVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /content/cats_and_dogs_filtered.zip\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seyq_VnYhk7g",
        "outputId": "8b950427-811e-4522-9a5a-6a95a609ab92"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-11 17:42:38--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.69.128, 64.233.183.128, 173.194.194.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.69.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/content/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/content/cats_and_d 100%[===================>]  65.43M   172MB/s    in 0.4s    \n",
            "\n",
            "2022-03-11 17:42:38 (172 MB/s) - ‘/content/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "NxDavT_Bh08y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tflite_model_path = '/content/mobilenet_v2_float32.tflite'\n",
        "\n",
        "#tflite_model_path = '/content/model_float32.tflite'\n",
        "# Load the TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "print(\"== Input details ==\")\n",
        "print(\"name:\", interpreter.get_input_details()[0]['name'])\n",
        "print(\"shape:\", interpreter.get_input_details()[0]['shape'])\n",
        "print(\"type:\", interpreter.get_input_details()[0]['dtype'])\n",
        "\n",
        "print(\"\\nDUMP INPUT\")\n",
        "print(interpreter.get_input_details()[0])\n",
        "\n",
        "print(\"\\n== Output details ==\")\n",
        "print(\"name:\", interpreter.get_output_details()[0]['name'])\n",
        "print(\"shape:\", interpreter.get_output_details()[0]['shape'])\n",
        "print(\"type:\", interpreter.get_output_details()[0]['dtype'])\n",
        "\n",
        "print(\"\\nDUMP OUTPUT\")\n",
        "print(interpreter.get_output_details()[0])\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model on image  data\n",
        "input_shape = input_details[0]['shape']\n",
        "#print(input_shape)\n",
        "image = tf.io.read_file('/content/cats_and_dogs_filtered/validation/cats/cat.2000.jpg')\n",
        "\n",
        "image = tf.io.decode_jpeg(image, channels=3)\n",
        "image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "image = tf.reshape(image,[3,IMAGE_SIZE,IMAGE_SIZE])\n",
        "image = tf.expand_dims(image, 0)\n",
        "print(\"Real image shape\")\n",
        "print(image.shape)\n",
        "#print(image)\n",
        "interpreter.set_tensor(input_details[0]['index'], image)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# get_tensor() returns a copy of the tensor data\n",
        "# use tensor() in order to get a pointer to the tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Predicted value . Label index: {}, confidence: {:2.0f}%\"\n",
        "      .format(np.argmax(output_data), \n",
        "              100 * output_data[0][np.argmax(output_data)]))"
      ],
      "metadata": {
        "id": "lhlCPBDO205F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e80dac4-deb7-4cab-e30a-7d894d6934f2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Input details ==\n",
            "name: serving_default_input:0\n",
            "shape: [  1   3 224 224]\n",
            "type: <class 'numpy.float32'>\n",
            "\n",
            "DUMP INPUT\n",
            "{'name': 'serving_default_input:0', 'index': 0, 'shape': array([  1,   3, 224, 224], dtype=int32), 'shape_signature': array([ -1,   3, 224, 224], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "\n",
            "== Output details ==\n",
            "name: PartitionedCall:0\n",
            "shape: [   1 1000]\n",
            "type: <class 'numpy.float32'>\n",
            "\n",
            "DUMP OUTPUT\n",
            "{'name': 'PartitionedCall:0', 'index': 346, 'shape': array([   1, 1000], dtype=int32), 'shape_signature': array([  -1, 1000], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "Real image shape\n",
            "(1, 3, 224, 224)\n",
            "Predicted value . Label index: 530, confidence: 1936%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Convert from TF saved model to TFLite(quantized) model"
      ],
      "metadata": {
        "id": "gys1dUne4SK0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9ydAmHGHUZl",
        "outputId": "fb65c14a-4d18-4a1f-c660-862ddf9268d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "# A generator that provides a representative dataset\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "saved_model_dir = 'model_tf'\n",
        "#flowers_dir = '/content/images'\n",
        "def representative_data_gen():\n",
        "  dataset_list = tf.data.Dataset.list_files('/content/cats_and_dogs_filtered/train' + '/*/*')\n",
        "  for i in range(1):\n",
        "    image = next(iter(dataset_list))\n",
        "    image = tf.io.read_file(image)\n",
        "    image = tf.io.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "    image = tf.reshape(image,[3,IMAGE_SIZE,IMAGE_SIZE])\n",
        "    image = tf.cast(image / 127., tf.float32)\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    print(image.shape)    \n",
        "    yield [image]\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "# Download an example image from the pytorch website\n",
        "import urllib\n",
        "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "def representative_data_gen_1():\n",
        "  dataset_list = tf.data.Dataset.list_files('/content/cats_and_dogs_filtered/train' + '/*/*')\n",
        "  for i in range(100):\n",
        "    input_image = next(iter(dataset_list))      \n",
        "    input_image = Image.open(filename)\n",
        "    preprocess = transforms.Compose([\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.Resize(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    #transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    input_tensor = preprocess(input_image)\n",
        "    print(input_tensor.shape)\n",
        "    input_tensor = tf.expand_dims(input_tensor, 0)\n",
        "    print(\"torch input_tensor size\")\n",
        "    print(input_tensor.shape)    \n",
        "    yield [input_tensor]   \n",
        "     \n",
        "#converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) \n",
        "# This enables quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# This sets the representative dataset for quantization\n",
        "converter.representative_dataset = representative_data_gen_1\n",
        "# This ensures that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "# These set the input and output tensors to uint8 (added in r2.3)\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('mobilenet_v2_1.0_224_quant.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run inference on TFLite(quantized) with image data"
      ],
      "metadata": {
        "id": "tY2yaCnBe6UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tflite_model_path = '/content/mobilenet_v2_1.0_224_quant.tflite'\n",
        "#tflite_model_path = '/content/model_float32.tflite'\n",
        "# Load the TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "print(\"== Input details ==\")\n",
        "print(\"name:\", interpreter.get_input_details()[0]['name'])\n",
        "print(\"shape:\", interpreter.get_input_details()[0]['shape'])\n",
        "print(\"type:\", interpreter.get_input_details()[0]['dtype'])\n",
        " \n",
        "\n",
        "print(\"\\nDUMP INPUT\")\n",
        "print(interpreter.get_input_details()[0])\n",
        "\n",
        "print(\"\\n== Output details ==\")\n",
        "print(\"name:\", interpreter.get_output_details()[0]['name'])\n",
        "print(\"shape:\", interpreter.get_output_details()[0]['shape'])\n",
        "print(\"type:\", interpreter.get_output_details()[0]['dtype'])\n",
        "\n",
        "print(\"\\nDUMP OUTPUT\")\n",
        "print(interpreter.get_output_details()[0])\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "test_details = interpreter.get_input_details()[0]\n",
        "scale, zero_point = test_details['quantization']\n",
        "print(scale)\n",
        "print(zero_point)\n",
        "# Test the model on image  data\n",
        "input_shape = input_details[0]['shape']\n",
        "#print(input_shape)\n",
        "#image = tf.io.read_file('/content/169371301_d9b91a2a42.jpg')\n",
        "#image = tf.io.read_file('/content/istockphoto-472306883-1024x1024.jpg')\n",
        "#image = tf.io.read_file('/content/images/Car-PNG-HD.png')\n",
        "image = tf.io.read_file('/content/cats_and_dogs_filtered/validation/cats/cat.2000.jpg')\n",
        "\n",
        "image = tf.io.decode_jpeg(image, channels=3)\n",
        "#print(image.shape)     \n",
        "image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "image = tf.reshape(image,[3,IMAGE_SIZE,IMAGE_SIZE])\n",
        "#print(image.shape)\n",
        "image = tf.cast(image / 127., tf.int8)\n",
        "#image = tf.cast(image , tf.float32)\n",
        "image = tf.expand_dims(image, 0)\n",
        "print(\"Real image shape\")\n",
        "print(image.shape)\n",
        "#image = np.int8(image)\n",
        "#print(image)\n",
        "#image = np.float32(image / scale + zero_point)\n",
        "interpreter.set_tensor(input_details[0]['index'], image)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# get_tensor() returns a copy of the tensor data\n",
        "# use tensor() in order to get a pointer to the tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Predicted value . Label index: {}, confidence: {:2.0f}%\"\n",
        "      .format(np.argmax(output_data), \n",
        "              100 * output_data[0][np.argmax(output_data)]))"
      ],
      "metadata": {
        "id": "MWAJCkXLOs4r",
        "outputId": "3a889493-8032-4858-a9d0-1fcf7f0bcac9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Input details ==\n",
            "name: serving_default_input:0\n",
            "shape: [  1   3 224 224]\n",
            "type: <class 'numpy.int8'>\n",
            "\n",
            "DUMP INPUT\n",
            "{'name': 'serving_default_input:0', 'index': 0, 'shape': array([  1,   3, 224, 224], dtype=int32), 'shape_signature': array([ -1,   3, 224, 224], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.020324693992733955, -8), 'quantization_parameters': {'scales': array([0.02032469], dtype=float32), 'zero_points': array([-8], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "\n",
            "== Output details ==\n",
            "name: PartitionedCall:0\n",
            "shape: [   1 1000]\n",
            "type: <class 'numpy.int8'>\n",
            "\n",
            "DUMP OUTPUT\n",
            "{'name': 'PartitionedCall:0', 'index': 380, 'shape': array([   1, 1000], dtype=int32), 'shape_signature': array([  -1, 1000], dtype=int32), 'dtype': <class 'numpy.int8'>, 'quantization': (0.10924528539180756, -48), 'quantization_parameters': {'scales': array([0.10924529], dtype=float32), 'zero_points': array([-48], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "0.020324693992733955\n",
            "-8\n",
            "Real image shape\n",
            "(1, 3, 224, 224)\n",
            "Predicted value . Label index: 554, confidence: 800%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run inference on TFLite(float32) model with dog.jpg\n",
        "\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\""
      ],
      "metadata": {
        "id": "FWGl9CYkO72I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download an example image from the pytorch website\n",
        "import urllib\n",
        "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "id": "C_vjTV0_Nlmv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "input_image = Image.open(filename)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "print(input_tensor.shape)\n",
        "#input_tensor = tf.reshape(input_tensor,[3,IMAGE_SIZE,IMAGE_SIZE])\n",
        "#input_tensor = tf.cast(input_tensor , tf.float32)\n",
        "input_tensor = tf.expand_dims(input_tensor, 0)\n",
        "print(\"torch input_tensor size\")\n",
        "print(input_tensor.shape)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tflite_model_path = '/content/mobilenet_v2_float32.tflite'\n",
        "# Load the TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model on random input data\n",
        "input_shape = input_details[0]['shape']\n",
        "print(input_shape)\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# get_tensor() returns a copy of the tensor data\n",
        "# use tensor() in order to get a pointer to the tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Predicted value for [0, 1] normalization. Label index: {}, confidence: {:2.0f}%\"\n",
        "      .format(np.argmax(output_data), \n",
        "              100 * output_data[0][np.argmax(output_data)]))"
      ],
      "metadata": {
        "id": "fBwumAcjNqIH",
        "outputId": "0368d2b4-230d-49ba-9f31-833ad393dd93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "[  1   3 224 224]\n",
            "Predicted value for [0, 1] normalization. Label index: 258, confidence: 1496%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run inference on TFLite(quantized) model with dog.jpg\n",
        "\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\""
      ],
      "metadata": {
        "id": "avOdAS7-aO2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download an example image from the pytorch website\n",
        "import urllib\n",
        "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tflite_model_path = '/content/mobilenet_v2_1.0_224_quant.tflite'\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "test_details = interpreter.get_input_details()[0]\n",
        "\n",
        "scale, zero_point = test_details['quantization']\n",
        "print(scale)\n",
        "print(zero_point)\n",
        "\n",
        "# Test the model on image  data\n",
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "input_image = Image.open(filename)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "print(input_tensor.shape)\n",
        "input_tensor = torch.unsqueeze(input_tensor, 0)\n",
        "input_tensor = torch.quantize_per_tensor(input_tensor, torch.tensor(scale), torch.tensor(zero_point), torch.qint8)\n",
        "input_tensor = torch.int_repr(input_tensor).numpy()\n",
        "\n",
        "print(\"torch input_tensor size:\")\n",
        "print(input_tensor.shape)\n",
        "print(input_tensor)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# get_tensor() returns a copy of the tensor data\n",
        "# use tensor() in order to get a pointer to the tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Predicted value . Label index: {}, confidence: {:2.0f}%\"\n",
        "      .format(np.argmax(output_data), \n",
        "              100 * output_data[0][np.argmax(output_data)]))"
      ],
      "metadata": {
        "id": "GoLMpH1QF4jc",
        "outputId": "64bd91a0-380b-480f-9678-1c6927601ca7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.020324693992733955\n",
            "-8\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size:\n",
            "(1, 3, 224, 224)\n",
            "[[[[-103 -103 -102 ... -108 -104 -103]\n",
            "   [-106 -101 -102 ... -105 -103  -95]\n",
            "   [-106 -104 -102 ... -109 -105 -101]\n",
            "   ...\n",
            "   [ -81  -87  -88 ...  -51  -62  -61]\n",
            "   [ -84  -87  -88 ...  -59  -88  -79]\n",
            "   [ -84  -81  -69 ...  -53  -71  -70]]\n",
            "\n",
            "  [[ -98  -98  -97 ... -106 -102 -102]\n",
            "   [ -98  -99  -99 ... -105 -103 -100]\n",
            "   [ -98  -99 -100 ... -105 -104 -102]\n",
            "   ...\n",
            "   [ -56  -56  -56 ...  -31  -39  -41]\n",
            "   [ -56  -56  -57 ...  -36  -63  -52]\n",
            "   [ -55  -56  -48 ...  -27  -48  -44]]\n",
            "\n",
            "  [[ -87  -86  -85 ...  -92  -92  -92]\n",
            "   [ -90  -88  -88 ...  -92  -92  -89]\n",
            "   [ -89  -90  -89 ...  -93  -92  -91]\n",
            "   ...\n",
            "   [ -73  -82  -86 ...  -52  -65  -62]\n",
            "   [ -72  -83  -87 ...  -55  -81  -76]\n",
            "   [ -71  -78  -68 ...  -44  -63  -71]]]]\n",
            "Predicted value . Label index: 258, confidence: 8600%\n"
          ]
        }
      ]
    }
  ]
}