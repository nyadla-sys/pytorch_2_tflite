{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_to_onnx_to_tflite(quantized)_with_imagedata.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aec064f2bbf34430a2363265e1964f51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b6d9e6dd370c483b8bdfcf2a6f2f36a1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a14f6d4978024fbfae5c7dda5e86167b",
              "IPY_MODEL_a13e446baf984b6db0d1e62ffbf59a2a",
              "IPY_MODEL_0682095378384f59a160105165834f53"
            ]
          }
        },
        "b6d9e6dd370c483b8bdfcf2a6f2f36a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a14f6d4978024fbfae5c7dda5e86167b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c898f1af0c96446cb56703768f4bdc1b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ea1ab4a849814698a0df49bb7e1ff1e9"
          }
        },
        "a13e446baf984b6db0d1e62ffbf59a2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b06d2480a44542e7a431ec2a1dd838a1",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 14212972,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 14212972,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d244f951365140d9be93fcfbffa3ee06"
          }
        },
        "0682095378384f59a160105165834f53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1c5195ba118a45e290b54051694bee5a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13.6M/13.6M [00:00&lt;00:00, 35.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb941cf923e2440d8ddaf810ad482b3a"
          }
        },
        "c898f1af0c96446cb56703768f4bdc1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ea1ab4a849814698a0df49bb7e1ff1e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b06d2480a44542e7a431ec2a1dd838a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d244f951365140d9be93fcfbffa3ee06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1c5195ba118a45e290b54051694bee5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb941cf923e2440d8ddaf810ad482b3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nyadla-sys/pytorch_2_tflite/blob/main/pytorch_to_onnx_to_tflite(quantized)_with_imagedata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install ONNX and ONNX runtime"
      ],
      "metadata": {
        "id": "OF42Y3YaWrbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "!pip install onnxruntime\n",
        "# Some standard imports\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.onnx\n",
        "import torchvision.models as models\n",
        "import onnx\n",
        "import onnxruntime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma98ov9Fzftx",
        "outputId": "894d15fb-577a-4c9f-956f-b91a384b8adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.11.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 8.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.10.0.2)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->onnx) (1.15.0)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.11.0\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (3.17.3)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (2.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (1.21.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime) (1.15.0)\n",
            "Installing collected packages: onnxruntime\n",
            "Successfully installed onnxruntime-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load mobilenetV2 from torch models"
      ],
      "metadata": {
        "id": "gL9nGbtlW9Wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.mobilenet_v2(pretrained=True)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "iVFwSNmHztHY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "aec064f2bbf34430a2363265e1964f51",
            "b6d9e6dd370c483b8bdfcf2a6f2f36a1",
            "a14f6d4978024fbfae5c7dda5e86167b",
            "a13e446baf984b6db0d1e62ffbf59a2a",
            "0682095378384f59a160105165834f53",
            "c898f1af0c96446cb56703768f4bdc1b",
            "ea1ab4a849814698a0df49bb7e1ff1e9",
            "b06d2480a44542e7a431ec2a1dd838a1",
            "d244f951365140d9be93fcfbffa3ee06",
            "1c5195ba118a45e290b54051694bee5a",
            "eb941cf923e2440d8ddaf810ad482b3a"
          ]
        },
        "outputId": "9666843d-4f21-48f0-ac39-b2b176cb2dae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/mobilenet_v2-b0353104.pth\" to /root/.cache/torch/hub/checkpoints/mobilenet_v2-b0353104.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aec064f2bbf34430a2363265e1964f51",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/13.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MobileNetV2(\n",
              "  (features): Sequential(\n",
              "    (0): ConvNormActivation(\n",
              "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "    (1): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (2): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
              "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (3): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (4): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (5): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (6): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (7): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
              "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (8): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (9): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (10): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (11): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
              "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (12): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (13): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (14): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (15): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (16): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (17): InvertedResidual(\n",
              "      (conv): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
              "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (18): ConvNormActivation(\n",
              "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU6(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.2, inplace=False)\n",
              "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert from pytorch to onnx"
      ],
      "metadata": {
        "id": "b4kR0xRGXOM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 1\n",
        "IMAGE_SIZE = 224\n",
        "# Input to the model\n",
        "x = torch.randn(BATCH_SIZE, 3, 224, 224, requires_grad=True)\n",
        "torch_out = model(x)\n",
        "\n",
        "# Export the model\n",
        "torch.onnx.export(model,                     # model being run\n",
        "                  x,                         # model input (or a tuple for multiple inputs)\n",
        "                  \"mobilenet_v2.onnx\",       # where to save the model (can be a file or file-like object)\n",
        "                  export_params=True,        # store the trained parameter weights inside the model file\n",
        "                  opset_version=10,          # the ONNX version to export the model to\n",
        "                  do_constant_folding=True,  # whether to execute constant folding for optimization\n",
        "                  input_names = ['input'],   # the model's input names\n",
        "                  output_names = ['output'], # the model's output names\n",
        "                  dynamic_axes={'input' : {0 : 'BATCH_SIZE'},    # variable length axes\n",
        "                                'output' : {0 : 'BATCH_SIZE'}})"
      ],
      "metadata": {
        "id": "FU91JNA9zvNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model = onnx.load(\"mobilenet_v2.onnx\")\n",
        "onnx.checker.check_model(onnx_model)"
      ],
      "metadata": {
        "id": "p2XeU2w2z294"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Compare ONNX Runtime and Pytorch results"
      ],
      "metadata": {
        "id": "tKkajaAyXjeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ort_session = onnxruntime.InferenceSession(\"mobilenet_v2.onnx\")\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "# compute ONNX Runtime output prediction\n",
        "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
        "ort_outs = ort_session.run(None, ort_inputs)\n",
        "\n",
        "# compare ONNX Runtime and PyTorch results\n",
        "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
        "\n",
        "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
      ],
      "metadata": {
        "id": "a0FGMK0Fz5yo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0fef2c7-5ad3-41ed-cb90-3d51188514f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert from Onnx to TF saved model"
      ],
      "metadata": {
        "id": "FQB1Q8JbX2As"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx-tf\n",
        "\n",
        "from onnx_tf.backend import prepare\n",
        "import onnx\n",
        "\n",
        "onnx_model_path = 'mobilenet_v2.onnx'\n",
        "tf_model_path = 'model_tf'\n",
        "\n",
        "onnx_model = onnx.load(onnx_model_path)\n",
        "tf_rep = prepare(onnx_model)\n",
        "tf_rep.export_graph(tf_model_path)"
      ],
      "metadata": {
        "id": "dBmONa3x1FZL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5869b44c-bc8d-4fea-dbb6-3ac72796478b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting onnx-tf\n",
            "  Downloading onnx_tf-1.9.0-py3-none-any.whl (222 kB)\n",
            "\u001b[K     |████████████████████████████████| 222 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 34.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from onnx-tf) (3.13)\n",
            "Requirement already satisfied: onnx>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from onnx-tf) (1.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.9.0->onnx-tf) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.9.0->onnx-tf) (1.21.5)\n",
            "Requirement already satisfied: protobuf>=3.12.2 in /usr/local/lib/python3.7/dist-packages (from onnx>=1.9.0->onnx-tf) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.2->onnx>=1.9.0->onnx-tf) (1.15.0)\n",
            "Requirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons->onnx-tf) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons, onnx-tf\n",
            "Successfully installed onnx-tf-1.9.0 tensorflow-addons-0.16.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gen_tensor_dict while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: model_tf/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: model_tf/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert from TF saved model to TFLite(float32) model"
      ],
      "metadata": {
        "id": "Waje2OHsX_Oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "saved_model_dir = 'model_tf'\n",
        "tflite_model_path = 'mobilenet_v2_float32.tflite'\n",
        "\n",
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model\n",
        "with open(tflite_model_path, 'wb') as f:\n",
        "    f.write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKq3zZfk1qwk",
        "outputId": "ff5ebe0e-a156-4e45-cc82-8500d485ab19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run inference on TFLite(float32) with random data"
      ],
      "metadata": {
        "id": "CsJDnIA1Yo8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tflite_model_path = '/content/mobilenet_v2_float32.tflite'\n",
        "# Load the TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model on random input data\n",
        "input_shape = input_details[0]['shape']\n",
        "print(input_shape)\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# get_tensor() returns a copy of the tensor data\n",
        "# use tensor() in order to get a pointer to the tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Predicted value for [0, 1] normalization. Label index: {}, confidence: {:2.0f}%\"\n",
        "      .format(np.argmax(output_data), \n",
        "              100 * output_data[0][np.argmax(output_data)]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD9dfODY2XH8",
        "outputId": "d8b0fa07-cf08-49ac-e6e0-cb4d489709ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  1   3 224 224]\n",
            "Predicted value for [0, 1] normalization. Label index: 892, confidence: 579%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run inference on TFLite(float32) with image data"
      ],
      "metadata": {
        "id": "jX0VA1kJbsVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "    -O /content/cats_and_dogs_filtered.zip\n",
        "\n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seyq_VnYhk7g",
        "outputId": "5117e476-575c-48fa-e82f-1d7d79dc719e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-04 05:09:25--  https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.201.128, 173.194.193.128, 173.194.196.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.201.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘/content/cats_and_dogs_filtered.zip’\n",
            "\n",
            "/content/cats_and_d 100%[===================>]  65.43M   128MB/s    in 0.5s    \n",
            "\n",
            "2022-03-04 05:09:25 (128 MB/s) - ‘/content/cats_and_dogs_filtered.zip’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/cats_and_dogs_filtered.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/content')\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "NxDavT_Bh08y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tflite_model_path = '/content/mobilenet_v2_float32.tflite'\n",
        "\n",
        "#tflite_model_path = '/content/model_float32.tflite'\n",
        "# Load the TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "print(\"== Input details ==\")\n",
        "print(\"name:\", interpreter.get_input_details()[0]['name'])\n",
        "print(\"shape:\", interpreter.get_input_details()[0]['shape'])\n",
        "print(\"type:\", interpreter.get_input_details()[0]['dtype'])\n",
        "\n",
        "print(\"\\nDUMP INPUT\")\n",
        "print(interpreter.get_input_details()[0])\n",
        "\n",
        "print(\"\\n== Output details ==\")\n",
        "print(\"name:\", interpreter.get_output_details()[0]['name'])\n",
        "print(\"shape:\", interpreter.get_output_details()[0]['shape'])\n",
        "print(\"type:\", interpreter.get_output_details()[0]['dtype'])\n",
        "\n",
        "print(\"\\nDUMP OUTPUT\")\n",
        "print(interpreter.get_output_details()[0])\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model on image  data\n",
        "input_shape = input_details[0]['shape']\n",
        "#print(input_shape)\n",
        "image = tf.io.read_file('/content/cats_and_dogs_filtered/validation/cats/cat.2000.jpg')\n",
        "\n",
        "image = tf.io.decode_jpeg(image, channels=3)\n",
        "image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "image = tf.reshape(image,[3,IMAGE_SIZE,IMAGE_SIZE])\n",
        "image = tf.expand_dims(image, 0)\n",
        "print(\"Real image shape\")\n",
        "print(image.shape)\n",
        "#print(image)\n",
        "interpreter.set_tensor(input_details[0]['index'], image)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# get_tensor() returns a copy of the tensor data\n",
        "# use tensor() in order to get a pointer to the tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Predicted value . Label index: {}, confidence: {:2.0f}%\"\n",
        "      .format(np.argmax(output_data), \n",
        "              100 * output_data[0][np.argmax(output_data)]))"
      ],
      "metadata": {
        "id": "lhlCPBDO205F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c883d4de-d349-4681-8e7f-c88d87a62b91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== Input details ==\n",
            "name: serving_default_input:0\n",
            "shape: [  1   3 224 224]\n",
            "type: <class 'numpy.float32'>\n",
            "\n",
            "DUMP INPUT\n",
            "{'name': 'serving_default_input:0', 'index': 0, 'shape': array([  1,   3, 224, 224], dtype=int32), 'shape_signature': array([ -1,   3, 224, 224], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "\n",
            "== Output details ==\n",
            "name: PartitionedCall:0\n",
            "shape: [   1 1000]\n",
            "type: <class 'numpy.float32'>\n",
            "\n",
            "DUMP OUTPUT\n",
            "{'name': 'PartitionedCall:0', 'index': 346, 'shape': array([   1, 1000], dtype=int32), 'shape_signature': array([  -1, 1000], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "Real image shape\n",
            "(1, 3, 224, 224)\n",
            "Predicted value . Label index: 530, confidence: 1936%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Convert from TF saved model to TFLite(quantized) model"
      ],
      "metadata": {
        "id": "gys1dUne4SK0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9ydAmHGHUZl",
        "outputId": "93d63ffd-8390-4970-be86-152e8c58309a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "# A generator that provides a representative dataset\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "saved_model_dir = 'model_tf'\n",
        "#flowers_dir = '/content/images'\n",
        "def representative_data_gen():\n",
        "  dataset_list = tf.data.Dataset.list_files('/content/cats_and_dogs_filtered/train' + '/*/*')\n",
        "  for i in range(1):\n",
        "    image = next(iter(dataset_list))\n",
        "    image = tf.io.read_file(image)\n",
        "    image = tf.io.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "    image = tf.reshape(image,[3,IMAGE_SIZE,IMAGE_SIZE])\n",
        "    image = tf.cast(image / 127., tf.float32)\n",
        "    image = tf.expand_dims(image, 0)\n",
        "    print(image.shape)    \n",
        "    yield [image]\n",
        "\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "# Download an example image from the pytorch website\n",
        "import urllib\n",
        "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "def representative_data_gen_1():\n",
        "  dataset_list = tf.data.Dataset.list_files('/content/cats_and_dogs_filtered/train' + '/*/*')\n",
        "  for i in range(100):\n",
        "    input_image = next(iter(dataset_list))      \n",
        "    input_image = Image.open(filename)\n",
        "    preprocess = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    input_tensor = preprocess(input_image)\n",
        "    print(input_tensor.shape)\n",
        "    input_tensor = tf.expand_dims(input_tensor, 0)\n",
        "    print(\"torch input_tensor size\")\n",
        "    print(input_tensor.shape)    \n",
        "    yield [input_tensor]   \n",
        "     \n",
        "#converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) \n",
        "# This enables quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "# This sets the representative dataset for quantization\n",
        "converter.representative_dataset = representative_data_gen_1\n",
        "# This ensures that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\n",
        "converter.target_spec.supported_types = [tf.int8]\n",
        "# These set the input and output tensors to uint8 (added in r2.3)\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('mobilenet_v2_1.0_224_quant.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run inference on TFLite(quantized) with image data"
      ],
      "metadata": {
        "id": "tY2yaCnBe6UO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tflite_model_path = '/content/mobilenet_v2_1.0_224_quant.tflite'\n",
        "#tflite_model_path = '/content/model_float32.tflite'\n",
        "# Load the TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "print(\"== Input details ==\")\n",
        "print(\"name:\", interpreter.get_input_details()[0]['name'])\n",
        "print(\"shape:\", interpreter.get_input_details()[0]['shape'])\n",
        "print(\"type:\", interpreter.get_input_details()[0]['dtype'])\n",
        " \n",
        "\n",
        "print(\"\\nDUMP INPUT\")\n",
        "print(interpreter.get_input_details()[0])\n",
        "\n",
        "print(\"\\n== Output details ==\")\n",
        "print(\"name:\", interpreter.get_output_details()[0]['name'])\n",
        "print(\"shape:\", interpreter.get_output_details()[0]['shape'])\n",
        "print(\"type:\", interpreter.get_output_details()[0]['dtype'])\n",
        "\n",
        "print(\"\\nDUMP OUTPUT\")\n",
        "print(interpreter.get_output_details()[0])\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "test_details = interpreter.get_input_details()[0]\n",
        "scale, zero_point = test_details['quantization']\n",
        "print(scale)\n",
        "print(zero_point)\n",
        "# Test the model on image  data\n",
        "input_shape = input_details[0]['shape']\n",
        "#print(input_shape)\n",
        "#image = tf.io.read_file('/content/169371301_d9b91a2a42.jpg')\n",
        "#image = tf.io.read_file('/content/istockphoto-472306883-1024x1024.jpg')\n",
        "#image = tf.io.read_file('/content/images/Car-PNG-HD.png')\n",
        "image = tf.io.read_file('/content/cats_and_dogs_filtered/validation/cats/cat.2000.jpg')\n",
        "\n",
        "image = tf.io.decode_jpeg(image, channels=3)\n",
        "#print(image.shape)     \n",
        "image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n",
        "image = tf.reshape(image,[3,IMAGE_SIZE,IMAGE_SIZE])\n",
        "#print(image.shape)\n",
        "image = tf.cast(image / 127., tf.int8)\n",
        "#image = tf.cast(image , tf.float32)\n",
        "image = tf.expand_dims(image, 0)\n",
        "print(\"Real image shape\")\n",
        "print(image.shape)\n",
        "#image = np.int8(image)\n",
        "#print(image)\n",
        "#image = np.float32(image / scale + zero_point)\n",
        "interpreter.set_tensor(input_details[0]['index'], image)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# get_tensor() returns a copy of the tensor data\n",
        "# use tensor() in order to get a pointer to the tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Predicted value . Label index: {}, confidence: {:2.0f}%\"\n",
        "      .format(np.argmax(output_data), \n",
        "              100 * output_data[0][np.argmax(output_data)]))"
      ],
      "metadata": {
        "id": "MWAJCkXLOs4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run inference on TFLite(float32) model with dog.jpg\n",
        "\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\""
      ],
      "metadata": {
        "id": "FWGl9CYkO72I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download an example image from the pytorch website\n",
        "import urllib\n",
        "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "id": "C_vjTV0_Nlmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "input_image = Image.open(filename)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "print(input_tensor.shape)\n",
        "#input_tensor = tf.reshape(input_tensor,[3,IMAGE_SIZE,IMAGE_SIZE])\n",
        "#input_tensor = tf.cast(input_tensor , tf.float32)\n",
        "input_tensor = tf.expand_dims(input_tensor, 0)\n",
        "print(\"torch input_tensor size\")\n",
        "print(input_tensor.shape)\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "tflite_model_path = '/content/mobilenet_v2_float32.tflite'\n",
        "# Load the TFLite model and allocate tensors\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model on random input data\n",
        "input_shape = input_details[0]['shape']\n",
        "print(input_shape)\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# get_tensor() returns a copy of the tensor data\n",
        "# use tensor() in order to get a pointer to the tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Predicted value for [0, 1] normalization. Label index: {}, confidence: {:2.0f}%\"\n",
        "      .format(np.argmax(output_data), \n",
        "              100 * output_data[0][np.argmax(output_data)]))"
      ],
      "metadata": {
        "id": "fBwumAcjNqIH",
        "outputId": "8a2506cc-dafc-4103-f11f-55921ffc7749",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "[  1   3 224 224]\n",
            "Predicted value for [0, 1] normalization. Label index: 258, confidence: 1436%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Run inference on TFLite(quantized) model with dog.jpg\n",
        "\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\""
      ],
      "metadata": {
        "id": "avOdAS7-aO2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "tflite_model_path = '/content/mobilenet_v2_1.0_224_quant.tflite'\n",
        "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
        "interpreter.allocate_tensors()\n",
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "input_image = Image.open(filename)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "print(input_tensor.shape)\n",
        "\n",
        "#input_tensor = tf.reshape(input_tensor,[3,IMAGE_SIZE,IMAGE_SIZE])\n",
        "#input_tensor = tf.cast(input_tensor , tf.float32)\n",
        "input_tensor = tf.expand_dims(input_tensor, 0)\n",
        "print(\"torch input_tensor size\")\n",
        "print(input_tensor.shape)\n",
        "\n",
        "# Get input and output tensors\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "test_details = interpreter.get_input_details()[0]\n",
        "\n",
        "# Test the model on image  data\n",
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "input_image = Image.open(filename)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "print(input_tensor.shape)\n",
        "input_tensor = np.int8(input_tensor)\n",
        "input_tensor = tf.expand_dims(input_tensor, 0)\n",
        "print(\"torch input_tensor size:\")\n",
        "print(input_tensor.shape)\n",
        "print(input_tensor)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_tensor)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# get_tensor() returns a copy of the tensor data\n",
        "# use tensor() in order to get a pointer to the tensor\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print(\"Predicted value . Label index: {}, confidence: {:2.0f}%\"\n",
        "      .format(np.argmax(output_data), \n",
        "              100 * output_data[0][np.argmax(output_data)]))"
      ],
      "metadata": {
        "id": "xJNp_99SPXdR",
        "outputId": "af970398-7099-478e-c7cf-34d29132ba3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size\n",
            "(1, 3, 224, 224)\n",
            "torch.Size([3, 224, 224])\n",
            "torch input_tensor size:\n",
            "(1, 3, 224, 224)\n",
            "tf.Tensor(\n",
            "[[[[-1 -1 -1 ... -2 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -2 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ...  0 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ...  0 -1 -1]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [ 0  0  0 ...  0  0  0]\n",
            "   [ 0  0 -1 ...  0 -1  0]\n",
            "   [ 0  0  0 ...  0  0  0]]\n",
            "\n",
            "  [[-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   [-1 -1 -1 ... -1 -1 -1]\n",
            "   ...\n",
            "   [-1 -1 -1 ...  0 -1 -1]\n",
            "   [-1 -1 -1 ...  0 -1 -1]\n",
            "   [-1 -1 -1 ...  0 -1 -1]]]], shape=(1, 3, 224, 224), dtype=int8)\n",
            "Predicted value . Label index: 701, confidence: 100%\n"
          ]
        }
      ]
    }
  ]
}